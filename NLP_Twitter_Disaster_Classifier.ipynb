{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nena00s/nena00s/blob/main/NLP_Twitter_Disaster_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Eu_NUp7VivDk"
      },
      "cell_type": "markdown",
      "source": [
        "## <span style='font-weight:bold;color:#560010'>INTRODUCTION</span>\n",
        "\n",
        "This notebook is a beginner kernel about NLP. For submission will evaluate different models based on Accuracy and F1-Score."
      ]
    },
    {
      "metadata": {
        "id": "QPMkCkRlivDm"
      },
      "cell_type": "markdown",
      "source": [
        "## <span style='font-weight:bold;color:#561225'>1.Import Libraries</span>"
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "iPGotCo3ivDn"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score , confusion_matrix, f1_score\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "import xgboost as xgboost\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "import seaborn as sns\n",
        "\n",
        "# Plotly\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "init_notebook_mode(connected=True) # #do not miss this line\n",
        "import plotly as py\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w7TUUeXuivDo"
      },
      "cell_type": "markdown",
      "source": [
        "## <span style='font-weight:bold;color:#561225'>2.Read Datas</span>"
      ]
    },
    {
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "z0V4KTs4ivDp"
      },
      "cell_type": "code",
      "source": [
        "data_train = pd.read_csv('../input/nlp-getting-started/train.csv')\n",
        "data_test = pd.read_csv('../input/nlp-getting-started/test.csv')\n",
        "submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cp4c7p6nivDp"
      },
      "cell_type": "markdown",
      "source": [
        "## <span style='font-weight:bold;color:#561225'>3.Data Analysis</span>"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "7d3sfO15ivDq"
      },
      "cell_type": "code",
      "source": [
        "display(data_train.head())\n",
        "display(data_test.head())\n",
        "display(submission.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "aPUB3LAJivDq"
      },
      "cell_type": "code",
      "source": [
        "data_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "9lUuT3EOivDr"
      },
      "cell_type": "code",
      "source": [
        "data_train.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sxGKYIC1ivDr"
      },
      "cell_type": "markdown",
      "source": [
        "### Columns\n",
        "<p><strong style='font-weight:bold;color:#561225'>id</strong>: a unique identifier for each tweet</p>\n",
        "<p><strong style='font-weight:bold;color:#561225'>text </strong>: the text of the tweet </p>\n",
        "<p><strong style='font-weight:bold;color:#561225'>location </strong>: the location the tweet was sent from (may be blank)</p>\n",
        "<p><strong style='font-weight:bold;color:#561225'>keyword </strong>: a particular keyword from the tweet (may be blank)</p>\n",
        "<p><strong style='font-weight:bold;color:#561225'>target </strong>: in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)</p>"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "tIF7-1daivDs"
      },
      "cell_type": "code",
      "source": [
        "print(\"Train set contains {} rows and {} cols\".format(data_train.shape[0],data_train.shape[1]))\n",
        "print(\"Test set contains {} rows and {} cols\".format(data_test.shape[0],data_test.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Fn-x-PUoivDt"
      },
      "cell_type": "code",
      "source": [
        "data_train.location.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "FvErCANOivDt"
      },
      "cell_type": "code",
      "source": [
        "data_train.keyword.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GkEsnm3JivDu"
      },
      "cell_type": "markdown",
      "source": [
        "### Missing Value"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fLbL5kdPivDu"
      },
      "cell_type": "code",
      "source": [
        "# Missing values in train set\n",
        "data_train.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "OVBXvvwbivDv"
      },
      "cell_type": "code",
      "source": [
        "# Missing values in test set\n",
        "data_test.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LloQbPkoivDv"
      },
      "cell_type": "markdown",
      "source": [
        "There are many missing values in the location column in both train and test sets."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "_ASXXkitivDv"
      },
      "cell_type": "code",
      "source": [
        "train = data_train[['text', 'target']]\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "gcfoQ0ijivDv"
      },
      "cell_type": "code",
      "source": [
        "train.target.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8bLdjE0iivDv"
      },
      "cell_type": "code",
      "source": [
        "test = data_test[['text']]\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9u7lH_ZLivDv"
      },
      "cell_type": "markdown",
      "source": [
        "## <span style='font-weight:bold;color:#561225'>4.Data Pre-Processing</span>\n",
        "\n",
        "Tweets are not syntactically well constructed. So preprocess will be applied.\n",
        "\n",
        "*  Convert all letters in tweets to lower case,\n",
        "*  Tokenization (disassembling according to desired features)\n",
        "*  Remove punctuation marks in the text,\n",
        "*  Removing Stopwords (commonly used words: the, at, and…)\n",
        "*  Removing URLs, mentions and usernames,\n",
        "*  Removing numerical expressions,"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "d7VXKAH5ivDw"
      },
      "cell_type": "code",
      "source": [
        "def clean_tweets(text):\n",
        "    text = re.sub('https?://[A-Za-z0-9./]*','', text) # Remove https..(URL)\n",
        "    text = re.sub('[0-9]*','', text) # Removed digits\n",
        "    text = re.sub('RT @[\\w]*:','', text) # Removed RT\n",
        "    text = re.sub('@[A-Za-z0-9]+', '', text) # Removed @mention\n",
        "    text = re.sub('&amp; ','',text) # Removed &(and)\n",
        "    return text\n",
        "\n",
        "def remove_punctuations(text):\n",
        "    text = ' '.join([i for i in text if i not in frozenset(string.punctuation)])\n",
        "    return text\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "stop_list = ['u','û_']\n",
        "for i in range(len(stop_list)):\n",
        "    stop.append(stop_list[i])\n",
        "\n",
        "def remove_stopword(text):\n",
        "    words = [w for w in text if w not in stop]\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "StOx1179ivDw"
      },
      "cell_type": "code",
      "source": [
        "train['cleaned_text'] = train['text'].apply(clean_tweets)\n",
        "train['cleaned_text'] = train['cleaned_text'].apply(lambda x: x.lower())\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "train['cleaned_text'] = train['cleaned_text'].apply(lambda x: tokenizer.tokenize(x)) # word tokenize\n",
        "train['cleaned_text'] = train['cleaned_text'].apply(remove_stopword)\n",
        "train['cleaned_text'] = train['cleaned_text'].apply(remove_punctuations)\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "EeZE-YfyivDw"
      },
      "cell_type": "code",
      "source": [
        "test['cleaned_text'] = test['text'].apply(clean_tweets)\n",
        "test['cleaned_text'] = test['cleaned_text'].apply(lambda x: x.lower())\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "test['cleaned_text'] = test['cleaned_text'].apply(lambda x: tokenizer.tokenize(x)) # word tokenize\n",
        "test['cleaned_text'] = test['cleaned_text'].apply(remove_stopword)\n",
        "test['cleaned_text'] = test['cleaned_text'].apply(remove_punctuations)\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RZvBzOXHivDw"
      },
      "cell_type": "markdown",
      "source": [
        "### Lemmatization\n",
        "\n",
        "<p><strong style='font-weight:bold;color:#561225'>Stemming,</strong> refers to reducing a word to its root form.\n",
        "\n",
        "<p><strong style='font-weight:bold;color:#561225'>Lemmatization,</strong> on the other hand, takes into consideration the morphological analysis of the words. <br>\n",
        "\n",
        "Stemming technique only looks at the form of the word whereas lemmatization technique looks at the meaning of the word.\n",
        "\n",
        "![](https://qph.fs.quoracdn.net/main-qimg-cd7f4bafaa42639deb999b1580bea69f)\n",
        "    \n",
        "Let's use lemmatization technique."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "NLIAvGGFivDx"
      },
      "cell_type": "code",
      "source": [
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "jAAeZAGYivDx"
      },
      "cell_type": "code",
      "source": [
        "train['cleaned_text'] = train['cleaned_text'].apply(lemmatize_text)\n",
        "train['cleaned_text'] = train['cleaned_text'].apply(remove_punctuations)\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "DJFDphw8ivDx"
      },
      "cell_type": "code",
      "source": [
        "test['cleaned_text'] = test['cleaned_text'].apply(lemmatize_text)\n",
        "test['cleaned_text'] = test['cleaned_text'].apply(remove_punctuations)\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WswNpnDFivDx"
      },
      "cell_type": "markdown",
      "source": [
        "## <span style='font-weight:bold;color:#561225'>5.Visualization</span>"
      ]
    },
    {
      "metadata": {
        "id": "157D2fZ5ivDx"
      },
      "cell_type": "markdown",
      "source": [
        "### The Target Distribution.\n",
        "\n",
        "We will see the target distribution with visualization"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "MXt7aQnEivDx"
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(5, 4), subplot_kw=dict(aspect=\"equal\"))\n",
        "labels=['Not-Disaster', 'Disaster']\n",
        "wedges, texts, autotexts = ax.pie(data_train.target.value_counts(),autopct=\"%1.2f%%\", colors=['#66b3ff','#cc1d00'],\n",
        "                                            explode = (0,0.07), startangle=90,\n",
        "                                            textprops={'fontsize': 15, 'color':'#f5f5f5'})\n",
        "plt.title('The Target Distribution', fontsize=16, weight=\"bold\")\n",
        "ax.legend(wedges, labels,\n",
        "          title=\"Ingredients\",\n",
        "          loc=\"center left\",\n",
        "          bbox_to_anchor=(1.2, 0, 0, 1))\n",
        "\n",
        "plt.setp(autotexts, weight=\"bold\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "w2HavZrEivDx"
      },
      "cell_type": "code",
      "source": [
        "fig = go.Figure([go.Bar(x=['Disaster', 'Not-Disaster'],\n",
        "                        y=[len(data_train[data_train['target']== 1]),len(data_train[data_train['target']== 0])])])\n",
        "fig.update_traces(marker_color='indianred', marker_line_color='rgb(58,48,107)',\n",
        "                  marker_line_width=1.5, opacity=0.7)\n",
        "fig.update_layout(title_text='The Target Distribution',autosize=False,width=400,height=500)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MQ03P6MzivDy"
      },
      "cell_type": "markdown",
      "source": [
        "We see that class 0 (Not-Disaster Tweets) is more than class 1 (Disaster Tweets)."
      ]
    },
    {
      "metadata": {
        "id": "-JH4kmCGivDy"
      },
      "cell_type": "markdown",
      "source": [
        "### WordCloud"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zBagTd0NivDy"
      },
      "cell_type": "code",
      "source": [
        "disaster = train[train['target']==1]['cleaned_text']\n",
        "non_disaster = train[train['target']==0]['cleaned_text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Q7BSf9MBivDy"
      },
      "cell_type": "code",
      "source": [
        "print('Disaster Tweet: {} \\nNot-Disaster Tweet: {}'.format(disaster.values[2],non_disaster.values[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "2btGhANOivDy"
      },
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "path = '../input/twitter-logo/twitter_logo.png'\n",
        "mask = np.array(Image.open(path).convert('L'))\n",
        "mask.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "LfGtUmLMivDy"
      },
      "cell_type": "code",
      "source": [
        "def grey_color_func(word, font_size, position, orientation, **kwargs):\n",
        "    return \"hsl(0, 0%%, %d%%)\" % np.random.randint(60, 100)\n",
        "\n",
        "fig, (plt1, plt2) = plt.subplots(1, 2, figsize=[14, 6])\n",
        "wordcloud = WordCloud(\n",
        "                        background_color='#123456',\n",
        "                        random_state = 42,\n",
        "                        max_words= 50,\n",
        "                        mask=mask,\n",
        "                        contour_width=1,\n",
        "                        contour_color=\"#b5b5b5\"\n",
        "                     ).generate(''.join(disaster))\n",
        "\n",
        "plt1.imshow(wordcloud.recolor(color_func=grey_color_func,random_state=3), interpolation=\"bilinear\")\n",
        "plt1.axis(\"off\")\n",
        "plt1.set_title('Disaster Tweets',fontsize=30);\n",
        "\n",
        "wordcloud = WordCloud(\n",
        "                        background_color='#123456',\n",
        "                        random_state = 42,\n",
        "                        max_words= 50,\n",
        "                        mask=mask,\n",
        "                        contour_width=1,\n",
        "                        contour_color=\"#b5b5b5\"\n",
        "                     ).generate(''.join(non_disaster))\n",
        "\n",
        "plt2.imshow(wordcloud.recolor(color_func=grey_color_func,random_state=3), interpolation=\"bilinear\")\n",
        "plt2.axis(\"off\")\n",
        "plt2.set_title('Not-Disaster Tweets',fontsize=30);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ePOUhS6civDz"
      },
      "cell_type": "markdown",
      "source": [
        "## <span style='font-weight:bold;color:#561225'>6.Converting Tokens to a Vector</span>"
      ]
    },
    {
      "metadata": {
        "id": "zDL39DdVivD3"
      },
      "cell_type": "markdown",
      "source": [
        "### Bag of Words"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "t9skQYXuivD3"
      },
      "cell_type": "code",
      "source": [
        "max_features=300\n",
        "count_vectorizer = CountVectorizer(max_features=max_features,stop_words=stop)\n",
        "train_vectors = count_vectorizer.fit_transform(train['text']).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "IVQ68qU_ivD3"
      },
      "cell_type": "code",
      "source": [
        "print('In Train Set, the most common {} words:\\n{} '.format(max_features,count_vectorizer.get_feature_names()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W1lO9tpcivD3"
      },
      "cell_type": "markdown",
      "source": [
        "### TF-IDF\n",
        "\n",
        "<i style='font-weight:bold;color:#561225'>TF: </i>Term Frequency, which measures how frequently a term occurs in a document.<br>\n",
        "* TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document). <br>\n",
        "\n",
        "<i style='font-weight:bold;color:#561225'>IDF: </i>Inverse Document Frequency, which measures how important a term is.<br>\n",
        "* IDF(t) = log_e(Total number of documents / Number of documents with term t in it)."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-z-hzWf0ivD4"
      },
      "cell_type": "code",
      "source": [
        "# Term Frequency\n",
        "tf = (train.cleaned_text).apply(lambda x : pd.value_counts(x.split(\" \"))).sum(axis=0).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "QFM2dD89ivD4"
      },
      "cell_type": "code",
      "source": [
        "tf.columns = ['words','frequence']\n",
        "tf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "CG1ANShCivD4"
      },
      "cell_type": "code",
      "source": [
        "tfreq = tf[tf['frequence']>100.0]\n",
        "plt.subplots(figsize = (18,5))\n",
        "chart = sns.barplot(x=tfreq.words, y=tfreq.frequence, palette=sns.color_palette(\"coolwarm\",7), edgecolor=\".3\")\n",
        "chart.set_xticklabels(chart.get_xticklabels(), rotation=75)\n",
        "chart.set_title('Frequencies of the Most Common Words');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8KyvQV8iivD4"
      },
      "cell_type": "code",
      "source": [
        "tf_idf_ngram = TfidfVectorizer(ngram_range=(1,2))\n",
        "tf_idf_ngram.fit(train.cleaned_text)\n",
        "x_train_tf_bigram = tf_idf_ngram.transform(train.cleaned_text) #.todense()\n",
        "x_test_tf_bigram = tf_idf_ngram.transform(test.cleaned_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "tpo43veMivD4"
      },
      "cell_type": "code",
      "source": [
        "print(x_train_tf_bigram.shape,x_test_tf_bigram.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "xL16IUDdivD4"
      },
      "cell_type": "code",
      "source": [
        "tf_idf_ngram.get_feature_names()[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Moi5aa-ivD4"
      },
      "cell_type": "markdown",
      "source": [
        "## <span style='font-weight:bold;color:#561225'>7. Text Classification Models</span>"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "FyEkNzN_ivD5"
      },
      "cell_type": "code",
      "source": [
        "X = x_train_tf_bigram\n",
        "y = train.target.values\n",
        "\n",
        "# Train-Test Splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print('Train Data splitted successfully')\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "wFghkXxPivD5"
      },
      "cell_type": "code",
      "source": [
        "df_accuracy = pd.DataFrame(columns=[\"Model\",\"Accuracy\",\"F1_score\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z8V2gwq3ivD5"
      },
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression Model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "1kqrTZ34ivD5"
      },
      "cell_type": "code",
      "source": [
        "# Fitting Train set\n",
        "clf_LR = LogisticRegression(C=2,dual=True, solver='liblinear',random_state=0)\n",
        "clf_LR.fit(X_train,y_train)\n",
        "\n",
        "# Predicting\n",
        "y_pred_LR = clf_LR.predict(X_test)\n",
        "\n",
        "# Calculating Model Accuracy and F1_score\n",
        "accuracy = accuracy_score(y_test, y_pred_LR) * 100\n",
        "f1score = f1_score(y_test, y_pred_LR) * 100\n",
        "print(\"Logistic Regression Accuracy: {0:.3f} %\".format(accuracy))\n",
        "print(\"Logistic Regression F1 Score: {0:.3f} %\".format(f1score))\n",
        "df_accuracy = df_accuracy.append({'Model':'LogisticRegression','Accuracy':accuracy, 'F1_score': f1score },ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7CjeWuQuivD5"
      },
      "cell_type": "markdown",
      "source": [
        "### Naive Bayes Model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "he2r1M0wivD9"
      },
      "cell_type": "code",
      "source": [
        "# Fitting Train set\n",
        "clf_NB = MultinomialNB()\n",
        "clf_NB.fit(X_train,y_train)\n",
        "\n",
        "# Predicting\n",
        "y_pred_NB = clf_NB.predict(X_test)\n",
        "\n",
        "# Calculating Model Accuracy and F1_score\n",
        "accuracy = accuracy_score(y_test, y_pred_NB) * 100\n",
        "f1score = f1_score(y_test, y_pred_NB) * 100\n",
        "print(\"MultinomialNB Accuracy: {0:.3f} %\".format(accuracy))\n",
        "print(\"MultinomialNB F1 Score: {0:.3f} %\".format(f1score))\n",
        "df_accuracy = df_accuracy.append({'Model':'NaiveBayes','Accuracy':accuracy, 'F1_score': f1score },ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TCr7InozivD-"
      },
      "cell_type": "markdown",
      "source": [
        "### K-Nearest Neighbors Model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "wIvLN6dzivD-"
      },
      "cell_type": "code",
      "source": [
        "# Fitting Train set\n",
        "clf_KNN = KNeighborsClassifier(n_neighbors = 7,weights = 'distance')\n",
        "clf_KNN.fit(X_train, y_train)\n",
        "\n",
        "# Predicting\n",
        "y_pred_KNN = clf_KNN.predict(X_test)\n",
        "\n",
        "# Calculating Model Accuracy and F1_score\n",
        "accuracy = accuracy_score(y_test, y_pred_KNN) * 100\n",
        "f1score = f1_score(y_test, y_pred_KNN) * 100\n",
        "print(\"K-Nearest Neighbors Accuracy: {0:.3f} %\".format(accuracy))\n",
        "print(\"K-Nearest Neighbors F1 Score: {0:.3f} %\".format(f1score))\n",
        "df_accuracy = df_accuracy.append({'Model':'K-NearestNeighbors','Accuracy':accuracy, 'F1_score': f1score },ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Bkdp-zRivD-"
      },
      "cell_type": "markdown",
      "source": [
        "### Random Forest Model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "UCizJmd7ivD-"
      },
      "cell_type": "code",
      "source": [
        "# Fitting Train set\n",
        "clf_RF = RandomForestClassifier(random_state=0)\n",
        "clf_RF.fit(X_train,y_train)\n",
        "\n",
        "# Predicting\n",
        "y_pred_RF = clf_RF.predict(X_test)\n",
        "\n",
        "# Calculating Model Accuracy and F1_score\n",
        "accuracy = accuracy_score(y_test, y_pred_RF) * 100\n",
        "f1score = f1_score(y_test, y_pred_RF) * 100\n",
        "print(\"Random Forest Accuracy: {0:.3f} %\".format(accuracy))\n",
        "print(\"Random Forest F1 Score: {0:.3f} %\".format(f1score))\n",
        "df_accuracy = df_accuracy.append({'Model':'RandomForest','Accuracy':accuracy, 'F1_score': f1score },ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lS8OpQ3_ivD-"
      },
      "cell_type": "markdown",
      "source": [
        "### Decision Tree Model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "6_iXpsOMivD-"
      },
      "cell_type": "code",
      "source": [
        "# Fitting Train set\n",
        "clf_DT = DecisionTreeClassifier(criterion= 'entropy', random_state=0)\n",
        "clf_DT.fit(X_train,y_train)\n",
        "\n",
        "# Predicting\n",
        "y_pred_DT = clf_DT.predict(X_test)\n",
        "\n",
        "# Calculating Model Accuracy and F1_score\n",
        "accuracy = accuracy_score(y_test, y_pred_DT) * 100\n",
        "f1score = f1_score(y_test, y_pred_DT) * 100\n",
        "print(\"Decision Tree Accuracy: {0:.3f} %\".format(accuracy))\n",
        "print(\"Decision Tree F1 Score: {0:.3f} %\".format(f1score))\n",
        "df_accuracy = df_accuracy.append({'Model':'DecisionTree','Accuracy':accuracy, 'F1_score': f1score },ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5NwCD9t4ivD_"
      },
      "cell_type": "markdown",
      "source": [
        "### Gradient Boosting Classifier Model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-N7Jmv_hivD_"
      },
      "cell_type": "code",
      "source": [
        "# Fitting Train set\n",
        "clf_GB = GradientBoostingClassifier(n_estimators=400, learning_rate=0.05, max_depth=20, random_state=0)\n",
        "clf_GB.fit(X_train,y_train)\n",
        "\n",
        "# Predicting\n",
        "y_pred_GB = clf_GB.predict(X_test)\n",
        "\n",
        "# Calculating Model Accuracy and F1_score\n",
        "accuracy = accuracy_score(y_test, y_pred_GB) * 100\n",
        "f1score = f1_score(y_test, y_pred_GB) * 100\n",
        "print(\"Gradient Boosting Classifier Accuracy: {0:.3f} %\".format(accuracy))\n",
        "print(\"Gradient Boosting Classifier F1 Score: {0:.3f} %\".format(f1score))\n",
        "df_accuracy = df_accuracy.append({'Model':'GradientBoostingClassifier','Accuracy':accuracy, 'F1_score': f1score },ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BcZWufT3ivD_"
      },
      "cell_type": "markdown",
      "source": [
        "### XGBOOST Classifier Model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "9Sx96r45ivD_"
      },
      "cell_type": "code",
      "source": [
        "# Fitting Train set\n",
        "clf_XGB = xgboost.XGBClassifier(n_estimators=400, random_state=0, learning_rate=0.05, booster=\"gbtree\",\n",
        "                                n_jobs=-1, max_depth=20)\n",
        "clf_XGB.fit(X_train,y_train)\n",
        "# Predicting\n",
        "y_pred_XGB = clf_XGB.predict(X_test)\n",
        "\n",
        "# Calculating Model Accuracy and F1_score\n",
        "accuracy = accuracy_score(y_test, y_pred_XGB) * 100\n",
        "f1score = f1_score(y_test, y_pred_XGB) * 100\n",
        "print(\"XGBOOST Classifier Accuracy: {0:.3f} %\".format(accuracy))\n",
        "print(\"XGBOOST Classifier F1 Score: {0:.3f} %\".format(f1score))\n",
        "df_accuracy = df_accuracy.append({'Model':'XGBOOSTClassifier','Accuracy':accuracy, 'F1_score': f1score },ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N1Lt_ed4ivD_"
      },
      "cell_type": "markdown",
      "source": [
        "### LightGB Classifier Model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "KF0TdMB6ivD_"
      },
      "cell_type": "code",
      "source": [
        "# Fitting Train set\n",
        "clf_LGB = LGBMClassifier(n_estimators=1300, learning_rate=0.05, random_state=0, max_depth=20, n_jobs=-1)\n",
        "clf_LGB.fit(X_train,y_train)\n",
        "\n",
        "# Predicting\n",
        "y_pred_LGB = clf_LGB.predict(X_test)\n",
        "\n",
        "# Calculating Model Accuracy and F1_score\n",
        "accuracy = accuracy_score(y_test, y_pred_LGB) * 100\n",
        "f1score = f1_score(y_test, y_pred_LGB) * 100\n",
        "print(\"LightGB Classifier Accuracy: {0:.3f} %\".format(accuracy))\n",
        "print(\"LightGB Classifier F1 Score: {0:.3f} %\".format(f1score))\n",
        "df_accuracy = df_accuracy.append({'Model':'LightGBClassifier','Accuracy':accuracy, 'F1_score': f1score},ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "kBmpbE2LivEA"
      },
      "cell_type": "code",
      "source": [
        "# Accuracy and F1-score Comparison of Models\n",
        "trace1=go.Bar(\n",
        "                x=df_accuracy.Model,\n",
        "                y=df_accuracy.Accuracy,\n",
        "                name=\"Accuracy\",\n",
        "                marker=dict(color = 'rgba(50, 240,120, 0.7)',\n",
        "                           line=dict(color='rgb(0,0,0)',width=1.9)),\n",
        "                text='Accuracy')\n",
        "trace2=go.Bar(\n",
        "                x=df_accuracy.Model,\n",
        "                y=df_accuracy.F1_score,\n",
        "                name=\"F1-score\",\n",
        "                marker=dict(color = 'rgba(240,120,10 , 0.7)',\n",
        "                           line=dict(color='rgb(0,0,0)',width=1.9)),\n",
        "                text='F1-score')\n",
        "\n",
        "edit_df=[trace1,trace2]\n",
        "layout=go.Layout(barmode=\"group\", xaxis_tickangle=-60, title=\"Accuracy and F1-score of Models\")\n",
        "fig=dict(data=edit_df,layout=layout)\n",
        "iplot(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IfC06ToUivEA"
      },
      "cell_type": "markdown",
      "source": [
        "In this kernel, 8 machine learning algorithms were used.As result;\n",
        "* The accuracy and the f1-score of Decision Tree Algorithm is lower than other algorithms.\n",
        "* The F1-score of the XGBOOST Algorithm is the highest compared to the others."
      ]
    },
    {
      "metadata": {
        "id": "ilL1L12SivEA"
      },
      "cell_type": "markdown",
      "source": [
        "## <span style='font-weight:bold;color:#561225'>8. Prediction and Submission</span>"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zFwlhFl2ivEA"
      },
      "cell_type": "code",
      "source": [
        "submission['target'] = clf_XGB.predict(x_test_tf_bigram)\n",
        "submission['target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "WjUlBiFcivEB"
      },
      "cell_type": "code",
      "source": [
        "submission_final= submission[['id','target']]\n",
        "submission_final.to_csv('submission.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HHoUvY97ivEB"
      },
      "cell_type": "markdown",
      "source": [
        "## <span style='font-weight:bold;color:#561225'>9. References</span>\n",
        "\n",
        "* https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove\n",
        "* https://www.kaggle.com/parulpandey/getting-started-with-nlp-a-general-intro\n",
        "* https://www.kaggle.com/kushbhatnagar/disaster-tweets-eda-nlp-classifier-models/notebook\n",
        "* https://www.kaggle.com/elcaiseri/nlp-the-simplest-way"
      ]
    },
    {
      "metadata": {
        "id": "FZtwCuS2ivEB"
      },
      "cell_type": "markdown",
      "source": [
        "<p style='font-weight:bold;color:#123456'><i>I hope you find this kernel useful. If you like it please do an upvote.</i><p>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}